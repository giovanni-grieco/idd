Query: {"query":{"multi_match":{"query":"Quantum Computing advancements","fields":["title^2","content"]}}}
{"took":6,"timed_out":false,"_shards":{"total":1,"successful":1,"skipped":0,"failed":0},"hits":{"total":{"value":2215,"relation":"eq"},"max_score":19.11413,"hits":[{"_index":"wikipedia","_type":"_doc","_id":"0iTVSZoBq7KcDrevBGce","_score":19.11413,"_source":{
  "content": "An advanced work, advance-work or advanced outwork is a fortification or outwork in front of the main defensive buildingCurl, James Stevens (2006). Oxford Dictionary of Architecture and Landscape Architecture, 2nd ed., OUP, Oxford and New York, p. 8. . or castle.Der Fortgürtel der Festung Ingolstadt ist einmalig. retrieved 27 October 2015 In the Middle Ages in the Holy Roman Empire, advanced works, known as Vorwerke (singular: Vorwerk), were commonly found in smaller villages that were located in front of the main castle. Within these advanced works often lived relatives of the knightly family whose ancestral seat was in the castle itself. As a result, the advanced works became manor houses and were known locally as schlosses. They were suitable for defending against minor attacks and offered the village population a degree of protection. In the case of major attacks they also acted as an early warning system for the castle. Because the advanced works were supposed to function autonomously, a link with agricultural estates was possible, such estates then became granges or vorwerkenden Gutshöfen. Later they also took over administrative tasks. Over the course of time these advanced works detached themselves from the castle and became independent estates. ==See also== * Folwark * List of established military terms == References == Category:Fortification (architectural elements) Category:Castle architecture ",
  "title": "Advanced work"
}},{"_index":"wikipedia","_type":"_doc","_id":"4CTZSZoBq7KcDrevP82I","_score":19.11413,"_source":{
  "content": "Time in Advance (no ISBN) is a collection of four short stories by American science fiction writer William Tenn (a pseudonym of Philip Klass). The stories all originally appeared in a number of different publications between 1952 and 1957. Time in Advance was first published by Bantam Books as a paperback in 1958 and also published as a hardcover in the United Kingdom by Victor Gollancz in 1963, followed a hardcover edition in 1964 published in the United Kingdom by the Science Fiction Book Club and by a Panther paperback edition in April 1966. ==Contents== Dedication: \"To Fruma: For being there during Winthrop at his worst and life at its best\" Fruma Klass was Philip Klass's (William Tenn's) wife, Winthrop being the name of the title character of the final story in the collection. ===\"Firewater\"=== (Astounding Science Fiction, February 1952) ====Plot==== The Earth is visited by large, enigmatic alien spheres, who take up residence in colonies on several prairies and deserts across the world. They make visits to cities, factories and other areas of human activity, seemingly to merely float and observe. All attempts at communication are unsuccessful and despite the best efforts of mankind, no one is able to decipher their intentions. Some, however, have come in to close encounter with the aliens, and emerged dramatically altered beings. These people, called humanity-prime, and dubbed 'primeys', are highly intelligent, can bend matter to their will, but are also, by human standards, quite, quite mad. Algernon Hebster is a highly successful businessman, owing mostly to his dealings with primeys, who supply him with the knowledge for advanced technologies which he puts to use in commerce. The problem is that primeys are so dangerous that dealing with them is highly illegal and every attempt is made to confine them to the reservations around their perceived alien masters. ===\"Time in Advance\"=== (Galaxy Science Fiction, August 1956) ====Plot==== In the far future a law is passed enabling citizens to serve out sentences for crimes they intend to commit, serving the full term, but with a 50% pre- criminal discount. Post-criminals and pre-criminals alike are sent to carry out hard-labour on hellishly perilous, far-flung Convict Planets. Few return. Those pre-criminals who are not killed, drop out before their terms are up, with nothing but scars and nightmares to show for their troubles. Two pre- criminals however, 'Blotto' Otto Henck and Nicholas Crandall, manage against all the odds to serve out two full terms for murder, and return to Earth as minor celebrities, with the right to kill one person each. Things, however, do not go quite as planned. Blotto Otto has his scheming wife in mind, only to find out she died the previous year in an unfortunate accident. For Crandall, whose life has been a perpetual series of failures, things go even worse. He intends to kill Frederick Stephenson, a man who stole his great invention. However, on his return, he receives a call from his terrified beloved ex-wife, who thinks she is his intended victim for her series of infidelities whilst they were married. Next he receives a call from his ex-business partner, pleading for his life because he thinks he is the intended victim for secretly cheating him out of vast sums of money. Crandall was previously unaware of either of these things. Still reeling, he meets his own brother, who thinks he is the intended victim, and reveals it was he with whom his wife was cheating. Finally, he calls his intended victim, Stephenson, the only one who fails to twist and squirm, but instead offers Crandall fair settlement for his invention. Shattered by the day's events, Crandall succumbs to the fact that he is one of life's born losers, and sets out with Otto to have some fun.\"Time in Advance\" at the Internet Archive ===\"The Sickness\"=== (Infinity Science Fiction, November 1955) ====Plot==== The Earth finds itself on the brink of catastrophic nuclear war between Russia and the United States. As a last-ditch symbolic gesture of peace and cooperation, the two nations, presided over by India, launch a joint manned venture to Mars. On their arrival to the red planet, Nicolai Belov, a Russian member of the crew, discovers a vast and amazing city once populated by human-like beings. However, once he returns to the ship he quickly develops a strange fever and is quarantined. This raises tensions in the already fraught atmosphere on board, and threatens to throw power amongst the crew out of balance. Equilibrium is restored, however, when American crew member Smathers also comes down with what is now dubbed Belov's disease. One by one the crew succumb, falling through several stages of fever and delirium, leaving prospects of return ever slimmer, and prospects of war on Earth ever greater: Mutual suspicion over the loss of the mission would trigger the final conflict. Soon just one man remains healthy, American astronaut O'Brien. Just when he thinks all is over, Belov and Smathers awake from their fevers, only they aren't quite the same. They have acquired super- human powers and intelligence, able to shape matter at will and communicate telepathically. O'Brien discovers that Belov's isn't a disease at all, but a fantastic symbiotic bacilli. Just when he realises that the problems of the Earth are over and a new era has dawned, Smathers reveals one final thing: Some people, like him, are naturally immune... Front cover of a Russian edition of \"Winthrop Was Stubborn\". ===\"Winthrop Was Stubborn\"=== (Galaxy Science Fiction, August 1957, published under the title \"Time Waits for Winthrop\") ====Plot==== The unstated present has been contacted by the future, when time travel is possible and hedonism is the norm. Five present individuals have been selected to travel to the future, while five compatible individuals have been selected to travel to the past. The compatibility of each time traveler to one traveling in the opposite directions is described as vital to the method, without a perfect balance of travelers it is impossible. The story opens when the present day travelers find themselves stranded in the future. The problem is the oldest of them, Winthrop, refuses to return to the past thus leaving all of them trapped. In the present he was merely a bum, but in the future he's a curio and encouraged to indulge his tastes to the point of gluttony. Each of them is forced to confront the part of the future they find the most distasteful. The first, an elderly lady, has to meet with Winthrop and plead with him to release them by returning. The scientist has to attend the great computer to seek advice on the situation, which he finds morally objectionable. In any case, the computer tells him to simply return to the others, as the story concludes with a twist.\"Time Waits for Winthrop\" at the Internet Archive ==Reception== Anthony Boucher received the collection enthusiastically, describing the two novellas included as \"models absolute of extrapolative with and insight\" while finding the shorter stories \"of almost comparable quality.\"\"Recommended Reading,\" F\u0026SF;, September 1958, p.98. ==Adaptations== The story \"Time in Advance\" was adapted in 1965 by Paul Erickson as one of twelve episodes of the first series of BBC anthology series Out of the Unknown. The episode survives, and has been released on DVD. ==References== ==External links== * * Out of the Unknown * Bibliography of William Tenn Category:1958 short story collections Category:Science fiction short story collections Category:Works by William Tenn ",
  "title": "Time in Advance"
}},{"_index":"wikipedia","_type":"_doc","_id":"YSTVSZoBq7KcDrevrnou","_score":17.055483,"_source":{
  "content": "Coherent control is a quantum mechanics-based method for controlling dynamical processes by light. The basic principle is to control quantum interference phenomena, typically by shaping the phase of laser pulses. The basic ideas have proliferated, finding vast application in spectroscopy mass spectra, quantum information processing, laser cooling, ultracold physics and more. ==Brief History== The initial idea was to control the outcome of chemical reactions. Two approaches were pursued: * in the time domain, a \"pump-dump\" scheme where the control is the time delay between pulses * in the frequency domain, interfering pathways controlled by one and three photons. The two basic methods eventually merged with the introduction of optimal control theory. Experimental realizations soon followed in the time domain and in the frequency domain. Two interlinked developments accelerated the field of coherent control: experimentally, it was the development of pulse shaping by a spatial light modulator Liquid Crystal Optically Addressed Spatial Light Modulator, *Slinger, C.; Cameron, C.; Stanley, M.; \"Computer-Generated Holography as a Generic Display Technology\" , IEEE Computer, Volume 38, Issue 8, Aug. 2005, pp 46–53 and its employment in coherent control. The second development was the idea of automatic feedback control and its experimental realization. ==Controllability== Coherent control aims to steer a quantum system from an initial state to a target state via an external field. For given initial and final (target) states, the coherent control is termed state- to-state control. A generalization is steering simultaneously an arbitrary set of initial pure states to an arbitrary set of final states i.e. controlling a unitary transformation. Such an application sets the foundation for a quantum gate operation. Controllability of a closed quantum system has been addressed by Tarn and Clark. Their theorem based in control theory states that for a finite-dimensional, closed-quantum system, the system is completely controllable, i.e. an arbitrary unitary transformation of the system can be realized by an appropriate application of the controls if the control operators and the unperturbed Hamiltonian generate the Lie algebra of all Hermitian operators. Complete controllability implies state-to-state controllability. The computational task of finding a control field for a particular state-to-state transformation is difficult and becomes more difficult with the increase in the size of the system. This task is in the class of hard inversion problems of high computational complexity. The algorithmic task of finding the field that generates a unitary transformation scales factorial more difficult with the size of the system. This is because a larger number of state-to-state control fields have to be found without interfering with the other control fields. Once constraints are imposed controllability can be degraded. For example, what is the minimum time required to achieve a control objective? This is termed the \"quantum speed limit\". ==Constructive approach to coherent control== The constructive approach uses a set of predetermined control fields for which the control outcome can be inferred. The pump dump scheme in the time domain and the three vs one photon interference scheme in the frequency domain are prime examples. Another constructive approach is based on adiabatic ideas. The most well studied method is Stimulated raman adiabatic passage STIRAP which employs an auxiliary state to achieve complete state-to-state population transfer. One of the most prolific generic pulse shapes is a chirped pulse a pulse with a varying frequency in time. ==Optimal control== Optimal control as applied in coherent control seeks the optimal control field for steering a quantum system to its objective. For state-to-state control the objective is defined as the maximum overlap at the final time T with the state |\\phi_f \\rangle : :J= |\\langle\\psi (T)| \\phi_f\\rangle|^2 where the initial state is | \\phi_i\\rangle. The time dependent control Hamiltonian has the typical form: : H(t) = H_0 + \\mu \\cdot \\epsilon(t) where \\epsilon (t) is the control field. Optimal control solves for the optimal field \\epsilon(t)using the calculus of variations introducing Lagrange multipliers. A new objective functional is defined : J' = J + \\int_0^{T} \\langle \\chi (t)|\\left( i \\frac{\\partial}{\\partial t}-H(\\epsilon(t))\\right)|\\psi(t) \\rangle dt +\\lambda \\int_o^T |\\epsilon(t)|^2 dt where |\\chi\\rangle is a wave function like Lagrange multiplier and the \\lambda parameter regulates the integral intensity. Variation of J' with respect to \\delta \\epsilon and \\delta \\psi leads to two coupled Schrödinger equations. A forward equation for |\\psi\\rangle with initial condition |\\psi(0)\\rangle=|\\phi_i\\rangleand a backward equation for the Lagrange multiplier |\\chi\\rangle with final condition |\\chi(T)\\rangle=|\\phi_f\\rangle. Finding a solution requires an iterative approach. Different algorithms have been applied for obtaining the control field such as the Krotov method. A local in time alternative method has been developed, where at each time step, the field is calculated to direct the state to the target. A related method has been called tracking ==Experimental applications== Some applications of coherent control are * Unimolecular and bimolecular chemical reactions. * The biological photoisomerization of Retinal. * The field of nuclear magnetic resonance. * The field of ultracold matter for photoassociation. * Laser cooling of internal degrees of freedom. * Quantum information processing.Larsen, T. W., K. D. Petersson, F. Kuemmeth, T. S. Jespersen, P. Krogstrup, and C. M. Marcus. \"Coherent control of a transmon qubit with a nanowire-based Josephson junction.\" Bulletin of the American Physical Society 60 (2015). * Attosecond physics. Another important issue is the spectral selectivity of two photon coherent control. These concepts can be applied to single pulse Raman spectroscopy and microscopy. As one of the cornerstones for enabling quantum technologies, optimal quantum control keeps evolving and expanding into areas as diverse as quantum-enhanced sensing, manipulation of single spins, photons, or atoms, optical spectroscopy, photochemistry, magnetic resonance (spectroscopy as well as medical imaging), quantum information processing, and quantum simulation. ==References== ==Further reading== *Principles of the Quantum Control of Molecular Processes, by Moshe Shapiro, Paul Brumer, pp. 250\\. . Wiley-VCH, (2003). *\"Quantum control of Molecular Processes\", Moshe Shapiro and Paul Brumer, Wiley-VCH (2012). *Rice, Stuart Alan, and Meishan Zhao. Optical control of molecular dynamics. New York: John Wiley, 2000. *d'Alessandro, Domenico. Introduction to quantum control and dynamics. CRC press, 2007. *David J. Tannor, \"Introduction to Quantum Mechanics: A Time- dependent Perspective\", (University Science Books, Sausalito, 2007). Category:Chemical reactions Category:Quantum mechanics Category:Control theory ",
  "title": "Coherent control"
}},{"_index":"wikipedia","_type":"_doc","_id":"0STVSZoBq7KcDrevBGcY","_score":16.343952,"_source":{
  "content": "National Skill Training Institute (Erstwhile ATI) in Calicut, Kerala, India, is a technical institute under the Directorate General of Training, Ministry of Skill Development and Entrepreneurship of the Government of India. It aims to develop skilled manpower for industry, instructional staff for the Industrial Training Institutes, and to upgrade the skill of in-service persons from industry. It is funded by the Government of India. The campus is situated near Govindapuram, a suburb of Kozhikode. The institute was established in 1981 as Model ITI (MITI), near Beypore, Calicut, with courses in Craftsmen Training in Restructured Pattern, with four trades. Later in 1997 it was shifted to a new building at Eravathukunnu, Govindapuram. At that time new trades were introduced and new hostel blocks also constructed.http://dget.gov.in/upload/files/5a86fdfab4e18DGTNewsletterDec2017Jan2018.pdf In 2014 MITI was upgraded to an advanced training institute. Long term courses offered by ATI include Craft Instructor Training Scheme (CITS) in the trades of mechanic, refrigeration and air conditioning, electrician, and electronic mechanic. In addition, ATI offers short term courses granting HVAC, CAD (Autocad \u0026 Catia), automation, automobile and electrical training. ==References == Category:Vocational education in India Category:Education in Kozhikode Category:1981 establishments in India ",
  "title": "Advanced Training Institute"
}},{"_index":"wikipedia","_type":"_doc","_id":"3iTVSZoBq7KcDrevsHqH","_score":15.682297,"_source":{
  "content": "Computational genomics (often referred to as Computational Genetics) refers to the use of computational and statistical analysis to decipher biology from genome sequences and related data, including both DNA and RNA sequence as well as other \"post-genomic\" data (i.e., experimental data obtained with technologies that require the genome sequence, such as genomic DNA microarrays). These, in combination with computational and statistical approaches to understanding the function of the genes and statistical association analysis, this field is also often referred to as Computational and Statistical Genetics/genomics. As such, computational genomics may be regarded as a subset of bioinformatics and computational biology, but with a focus on using whole genomes (rather than individual genes) to understand the principles of how the DNA of a species controls its biology at the molecular level and beyond. With the current abundance of massive biological datasets, computational studies have become one of the most important means to biological discovery.Computational Genomics and Proteomics at MIT ==History== The roots of computational genomics are shared with those of bioinformatics. During the 1960s, Margaret Dayhoff and others at the National Biomedical Research Foundation assembled databases of homologous protein sequences for evolutionary study. Their research developed a phylogenetic tree that determined the evolutionary changes that were required for a particular protein to change into another protein based on the underlying amino acid sequences. This led them to create a scoring matrix that assessed the likelihood of one protein being related to another. Beginning in the 1980s, databases of genome sequences began to be recorded, but this presented new challenges in the form of searching and comparing the databases of gene information. Unlike text-searching algorithms that are used on websites such as Google or Wikipedia, searching for sections of genetic similarity requires one to find strings that are not simply identical, but similar. This led to the development of the Needleman-Wunsch algorithm, which is a dynamic programming algorithm for comparing sets of amino acid sequences with each other by using scoring matrices derived from the earlier research by Dayhoff. Later, the BLAST algorithm was developed for performing fast, optimized searches of gene sequence databases. BLAST and its derivatives are probably the most widely used algorithms for this purpose. The emergence of the phrase \"computational genomics\" coincides with the availability of complete sequenced genomes in the mid-to-late 1990s. The first meeting of the Annual Conference on Computational Genomics was organized by scientists from The Institute for Genomic Research (TIGR) in 1998, providing a forum for this speciality and effectively distinguishing this area of science from the more general fields of Genomics or Computational Biology. The first use of this term in scientific literature, according to MEDLINE abstracts, was just one year earlier in Nucleic Acids Research. The final Computational Genomics conference was held in 2006, featuring a keynote talk by Nobel Laureate Barry Marshall, co- discoverer of the link between Helicobacter pylori and stomach ulcers. As of 2014, the leading conferences in the field include Intelligent Systems for Molecular Biology (ISMB) and Research in Computational Molecular Biology (RECOMB). The development of computer-assisted mathematics (using products such as Mathematica or Matlab) has helped engineers, mathematicians and computer scientists to start operating in this domain, and a public collection of case studies and demonstrations is growing, ranging from whole genome comparisons to gene expression analysis. This has increased the introduction of different ideas, including concepts from systems and control, information theory, strings analysis and data mining. It is anticipated that computational approaches will become and remain a standard topic for research and teaching, while students fluent in both topics start being formed in the multiple courses created in the past few years. ==Contributions of computational genomics research to biology== Contributions of computational genomics research to biology include: * proposing cellular signalling networks * proposing mechanisms of genome evolution * predict precise locations of all human genes using comparative genomics techniques with several mammalian and vertebrate species * predict conserved genomic regions that are related to early embryonic development * discover potential links between repeated sequence motifs and tissue-specific gene expression * measure regions of genomes that have undergone unusually rapid evolution == See also == *Bioinformatics *Computational biology *Genomics *Microarray *BLAST *Computational epigenetics == References== == External links == * Harvard Extension School Biophysics 101, Genomics and Computational Biology, http://www.courses.fas.harvard.edu/~bphys101/info/syllabus.html * University of Bristol course in Computational Genomics, http://www.computational- genomics.net/ Category:Bioinformatics Category:Omics Category:Genomics Category:Computational fields of study ",
  "title": "Computational genomics"
}},{"_index":"wikipedia","_type":"_doc","_id":"6iTVSZoBq7KcDrevsHrA","_score":15.682297,"_source":{
  "content": "Concurrent computing is a form of computing in which several computations are executed concurrently—during overlapping time periods—instead of sequentially, with one completing before the next starts. This is a property of a system—whether a program, computer, or a network—where there is a separate execution point or \"thread of control\" for each process. A concurrent system is one where a computation can advance without waiting for all other computations to complete.Operating System Concepts 9th edition, Abraham Silberschatz. \"Chapter 4: Threads\" Concurrent computing is a form of modular programming. In its paradigm an overall computation is factored into subcomputations that may be executed concurrently. Pioneers in the field of concurrent computing include Edsger Dijkstra, Per Brinch Hansen, and C.A.R. Hoare. ==Introduction== The concept of concurrent computing is frequently confused with the related but distinct concept of parallel computing,Pike, Rob (2012-01-11). \"Concurrency is not Parallelism\". Waza conference, 11 January 2012. Retrieved from http://talks.golang.org/2012/waza.slide (slides) and http://vimeo.com/49718712 (video). although both can be described as \"multiple processes executing during the same period of time\". In parallel computing, execution occurs at the same physical instant: for example, on separate processors of a multi-processor machine, with the goal of speeding up computations—parallel computing is impossible on a (one-core) single processor, as only one computation can occur at any instant (during any single clock cycle). By contrast, concurrent computing consists of process lifetimes overlapping, but execution need not happen at the same instant. The goal here is to model processes in the outside world that happen concurrently, such as multiple clients accessing a server at the same time. Structuring software systems as composed of multiple concurrent, communicating parts can be useful for tackling complexity, regardless of whether the parts can be executed in parallel. For example, concurrent processes can be executed on one core by interleaving the execution steps of each process via time-sharing slices: only one process runs at a time, and if it does not complete during its time slice, it is paused, another process begins or resumes, and then later the original process is resumed. In this way, multiple processes are part-way through execution at a single instant, but only one process is being executed at that instant. Concurrent computations may be executed in parallel, for example, by assigning each process to a separate processor or processor core, or distributing a computation across a network. In general, however, the languages, tools, and techniques for parallel programming might not be suitable for concurrent programming, and vice versa. The exact timing of when tasks in a concurrent system are executed depend on the scheduling, and tasks need not always be executed concurrently. For example, given two tasks, T1 and T2: * T1 may be executed and finished before T2 or vice versa (serial and sequential) * T1 and T2 may be executed alternately (serial and concurrent) * T1 and T2 may be executed simultaneously at the same instant of time (parallel and concurrent) The word \"sequential\" is used as an antonym for both \"concurrent\" and \"parallel\"; when these are explicitly distinguished, concurrent/sequential and parallel/serial are used as opposing pairs. A schedule in which tasks execute one at a time (serially, no parallelism), without interleaving (sequentially, no concurrency: no task begins until the prior task ends) is called a serial schedule. A set of tasks that can be scheduled serially is serializable, which simplifies concurrency control. ===Coordinating access to shared resources=== The main challenge in designing concurrent programs is concurrency control: ensuring the correct sequencing of the interactions or communications between different computational executions, and coordinating access to resources that are shared among executions. Potential problems include race conditions, deadlocks, and resource starvation. For example, consider the following algorithm to make withdrawals from a checking account represented by the shared resource `balance`: bool withdraw(int withdrawal) { if (balance \u003e= withdrawal) { balance -= withdrawal; return true; } return false; } Suppose `balance = 500`, and two concurrent threads make the calls `withdraw(300)` and `withdraw(350)`. If line 3 in both operations executes before line 5 both operations will find that `balance \u003e= withdrawal` evaluates to `true`, and execution will proceed to subtracting the withdrawal amount. However, since both processes perform their withdrawals, the total amount withdrawn will end up being more than the original balance. These sorts of problems with shared resources benefit from the use of concurrency control, or non-blocking algorithms. ===Advantages=== The advantages of concurrent computing include: * Increased program throughput—parallel execution of a concurrent program allows the number of tasks completed in a given time to increase proportionally to the number of processors according to Gustafson's law * High responsiveness for input/output—input/output-intensive programs mostly wait for input or output operations to complete. Concurrent programming allows the time that would be spent waiting to be used for another task. * More appropriate program structure—some problems and problem domains are well-suited to representation as concurrent tasks or processes. ==Models== Models for understanding and analyzing concurrent computing systems include: *Actor model ** Object- capability model for security *Input/output automaton *Software transactional memory (STM) *Petri nets *Process calculi such as **Ambient calculus **Calculus of communicating systems (CCS) **Communicating sequential processes (CSP) **Join-calculus **π-calculus ==Implementation== A number of different methods can be used to implement concurrent programs, such as implementing each computational execution as an operating system process, or implementing the computational processes as a set of threads within a single operating system process. ===Interaction and communication=== In some concurrent computing systems, communication between the concurrent components is hidden from the programmer (e.g., by using futures), while in others it must be handled explicitly. Explicit communication can be divided into two classes: ;Shared memory communication: Concurrent components communicate by altering the contents of shared memory locations (exemplified by Java and C#). This style of concurrent programming usually needs the use of some form of locking (e.g., mutexes, semaphores, or monitors) to coordinate between threads. A program that properly implements any of these is said to be thread-safe. ;Message passing communication: Concurrent components communicate by exchanging messages (exemplified by MPI, Go, Scala, Erlang and occam). The exchange of messages may be carried out asynchronously, or may use a synchronous \"rendezvous\" style in which the sender blocks until the message is received. Asynchronous message passing may be reliable or unreliable (sometimes referred to as \"send and pray\"). Message-passing concurrency tends to be far easier to reason about than shared-memory concurrency, and is typically considered a more robust form of concurrent programming. A wide variety of mathematical theories to understand and analyze message-passing systems are available, including the actor model, and various process calculi. Message passing can be efficiently implemented via symmetric multiprocessing, with or without shared memory cache coherence. Shared memory and message passing concurrency have different performance characteristics. Typically (although not always), the per-process memory overhead and task switching overhead is lower in a message passing system, but the overhead of message passing is greater than for a procedure call. These differences are often overwhelmed by other performance factors. ==History== Concurrent computing developed out of earlier work on railroads and telegraphy, from the 19th and early 20th century, and some terms date to this period, such as semaphores. These arose to address the question of how to handle multiple trains on the same railroad system (avoiding collisions and maximizing efficiency) and how to handle multiple transmissions over a given set of wires (improving efficiency), such as via time-division multiplexing (1870s). The academic study of concurrent algorithms started in the 1960s, with credited with being the first paper in this field, identifying and solving mutual exclusion. ==Prevalence== Concurrency is pervasive in computing, occurring from low-level hardware on a single chip to worldwide networks. Examples follow. At the programming language level: * Channel * Coroutine * Futures and promises At the operating system level: * Computer multitasking, including both cooperative multitasking and preemptive multitasking ** Time-sharing, which replaced sequential batch processing of jobs with concurrent use of a system * Process * Thread At the network level, networked systems are generally concurrent by their nature, as they consist of separate devices. ==Languages supporting concurrent programming== Concurrent programming languages are programming languages that use language constructs for concurrency. These constructs may involve multi-threading, support for distributed computing, message passing, shared resources (including shared memory) or futures and promises. Such languages are sometimes described as concurrency-oriented languages or concurrency-oriented programming languages (COPL). Today, the most commonly used programming languages that have specific constructs for concurrency are Java and C#. Both of these languages fundamentally use a shared-memory concurrency model, with locking provided by monitors (although message-passing models can and have been implemented on top of the underlying shared-memory model). Of the languages that use a message-passing concurrency model, Erlang is probably the most widely used in industry at present. Many concurrent programming languages have been developed more as research languages (e.g. Pict) rather than as languages for production use. However, languages such as Erlang, Limbo, and occam have seen industrial use at various times in the last 20 years. Languages in which concurrency plays an important role include: * Ada—general purpose, with native support for message passing and monitor based concurrency * Alef—concurrent, with threads and message passing, for system programming in early versions of Plan 9 from Bell Labs * Alice—extension to Standard ML, adds support for concurrency via futures * Ateji PX—extension to Java with parallel primitives inspired from π-calculus * Axum—domain specific, concurrent, based on actor model and .NET Common Language Runtime using a C-like syntax * BMDFM—Binary Modular DataFlow Machine * C++—std::thread * Cω (C omega)—for research, extends C#, uses asynchronous communication * C#—supports concurrent computing using lock, yield, also since version 5.0 async and await keywords introduced * Clojure—modern, functional dialect of Lisp on the Java platform * Concurrent Clean—functional programming, similar to Haskell * Concurrent Collections (CnC)—Achieves implicit parallelism independent of memory model by explicitly defining flow of data and control * Concurrent Haskell—lazy, pure functional language operating concurrent processes on shared memory * Concurrent ML—concurrent extension of Standard ML * Concurrent Pascal—by Per Brinch Hansen * Curry * D—multi-paradigm system programming language with explicit support for concurrent programming (actor model) * E—uses promises to preclude deadlocks * ECMAScript—uses promises for asynchronous operations * Eiffel—through its SCOOP mechanism based on the concepts of Design by Contract * Elixir—dynamic and functional meta-programming aware language running on the Erlang VM. * Erlang—uses asynchronous message passing with nothing shared * FAUST—real-time functional, for signal processing, compiler provides automatic parallelization via OpenMP or a specific work-stealing scheduler * Fortran—coarrays and do concurrent are part of Fortran 2008 standard * Go—for system programming, with a concurrent programming model based on CSP * Haskell—concurrent, and parallel functional programming language Marlow, Simon (2013) Parallel and Concurrent Programming in Haskell : Techniques for Multicore and Multithreaded Programming * Hume—functional, concurrent, for bounded space and time environments where automata processes are described by synchronous channels patterns and message passing * Io—actor-based concurrency * Janus—features distinct askers and tellers to logical variables, bag channels; is purely declarative * Java—thread class or Runnable interface * Julia—\"concurrent programming primitives: Tasks, async-wait, Channels.\"https://juliacon.talkfunnel.com/2015/21-concurrent-and-parallel- programming-in-julia Concurrent and Parallel programming in Julia * JavaScript—via web workers, in a browser environment, promises, and callbacks. * JoCaml—concurrent and distributed channel based, extension of OCaml, implements the join-calculus of processes * Join Java—concurrent, based on Java language * Joule—dataflow-based, communicates by message passing * Joyce—concurrent, teaching, built on Concurrent Pascal with features from CSP by Per Brinch Hansen * LabVIEW—graphical, dataflow, functions are nodes in a graph, data is wires between the nodes; includes object-oriented language * Limbo—relative of Alef, for system programming in Inferno (operating system) * MultiLisp—Scheme variant extended to support parallelism * Modula-2—for system programming, by N. Wirth as a successor to Pascal with native support for coroutines * Modula-3—modern member of Algol family with extensive support for threads, mutexes, condition variables * Newsqueak—for research, with channels as first-class values; predecessor of Alef * occam—influenced heavily by communicating sequential processes (CSP) ** occam-π—a modern variant of occam, which incorporates ideas from Milner's π-calculus * Orc—heavily concurrent, nondeterministic, based on Kleene algebra * Oz-Mozart—multiparadigm, supports shared-state and message-passing concurrency, and futures * ParaSail—object- oriented, parallel, free of pointers, race conditions * Pict—essentially an executable implementation of Milner's π-calculus *Raku includes classes for threads, promises and channels by default * Python using Stackless Python * Reia—uses asynchronous message passing between shared-nothing objects * Red/System—for system programming, based on Rebol * Rust—for system programming, using message-passing with move semantics, shared immutable memory, and shared mutable memory. * Scala—general purpose, designed to express common programming patterns in a concise, elegant, and type-safe way * SequenceL—general purpose functional, main design objectives are ease of programming, code clarity-readability, and automatic parallelization for performance on multicore hardware, and provably free of race conditions * SR—for research * SuperPascal—concurrent, for teaching, built on Concurrent Pascal and Joyce by Per Brinch Hansen * Unicon—for research * TNSDL—for developing telecommunication exchanges, uses asynchronous message passing * VHSIC Hardware Description Language (VHDL)—IEEE STD-1076 * XC—concurrency- extended subset of C language developed by XMOS, based on communicating sequential processes, built-in constructs for programmable I/O Many other languages provide support for concurrency in the form of libraries, at levels roughly comparable with the above list. ==See also== * Asynchronous I/O * Chu space * Flow-based programming * Java ConcurrentMap * List of important publications in concurrent, parallel, and distributed computing * Ptolemy Project * * Sheaf (mathematics) * Transaction processing ==Notes== ==References== ==Sources== * ==Further reading== * * * * * * ==External links== * *Concurrent Systems Virtual Library Category:Operating system technology Category:Edsger W. Dijkstra Category:Dutch inventions ",
  "title": "Concurrent computing"
}},{"_index":"wikipedia","_type":"_doc","_id":"xyTVSZoBq7KcDrevyH_G","_score":15.682297,"_source":{
  "content": "Dirty Computer is the third studio album by American singer Janelle Monáe, released on April 27, 2018 by Wondaland Arts Society, Bad Boy Records and Atlantic Records. It is the follow-up to her studio albums The ArchAndroid (2010) and The Electric Lady (2013) and her first album not to be a part of Cindi Mayweather's Metropolis narrative. A departure from the more psychedelic sound of her early work, Dirty Computer is a pop, funk, hip hop, R\u0026B;, and neo soul record, featuring elements of electropop, space rock, pop rock, Minneapolis soul, trap, futurepop, new wave, synthpop, and latin music. Four singles, \"Make Me Feel\", \"Django Jane\", \"Pynk\", and \"I Like That\", were chosen to promote the album. Its release was accompanied by a 46-minute narrative film project of the same name. The album received widespread critical acclaim upon release; it was included in the top three of nine publications' Best Album of 2018 lists, and received two nominations at the 61st Annual Grammy Awards, including Album of the Year. It debuted at number six on the Billboard 200 and was further promoted by Monae's Dirty Computer Tour. ==Background and recording== In October 2016, Monáe made her feature-length film acting debut in Moonlight, alongside Naomie Harris, André Holland, and Mahershala Ali. Monáe also starred in the film Hidden Figures, alongside actresses Taraji P. Henson and Octavia Spencer; the film was released in December 2016. While filming her two movie roles, Monáe remained active in music with features on Grimes' \"Venus Fly\" from her Art Angels album and also the soundtrack for the Netflix series The Get Down with a song titled, \"Hum Along and Dance (Gotta Get Down)\". She was also on the tracks \"Isn't This the World\" and \"Jalapeño\" for the Hidden Figures soundtrack. In an interview with People, Monáe revealed that she was already working on her third studio album when she received the scripts for her two first acting roles; therefore, she put the album on hold. She also revealed in the interview that she would be releasing new music sometime in 2017, although by the end of the year no album or single was announced. It was confirmed by Monáe after \"Make Me Feel\" was released that Prince, with whom she collaborated on her preceding album, had worked on the single, as well as the entire album, before his death in 2016. This was confirmed after listeners noticed similarities between the single's sound and the late musician's work. Monae stated in an interview with BBC Radio 1: \"Prince was actually working on the album with me before he passed on to another frequency, and helped me come up with some sounds. And I really miss him, you know, it’s hard for me to talk about him. But I do miss him, and his spirit will never leave me.\" The synth groove in \"Make Me Feel\", reported to be written by Prince, was played at one of his parties years prior to its inclusion in the single, as confirmed by Prince's DJ, Lenka Paris. Monáe had been exploring the themes presented in Dirty Computer for a decade before its production, but noted that \"earlier it felt safer to package herself in metaphors...The sanitized android version felt more accepted — and more acceptable — than her true self. The public, she explained, doesn't really 'know Janelle Monáe, and I felt like I didn't really have to be her because they were fine with Cindi.'\" Monáe considers Dirty Computer to be \"a homage to women and the spectrum of sexual identities.\" The album's 14 tracks can be grouped into three loose categories: Reckoning, Celebration and Reclamation. The first deals with Monáe's recognition of how she is viewed by society, the middle explores her acceptance of \"the cards she has been dealt\", and the closing tracks deal with her reclamation and redefinition of American identity. Overall, the album is Monáe's attempt to \"step into a more authentic self\". ==Release and promotion== On February 16, 2018, Monáe revealed her third studio album and the accompanying narrative film, entitled Dirty Computer, through a teaser video released on YouTube. The teaser video aired nationwide in select theaters prior to screenings of Black Panther. Monáe held a series of \"top-secret\" listening sessions in Los Angeles and New York prior to the album announcement. The album was released on April 27, 2018. It was supported with the simultaneous release of a 46-minute short film of the same name, dubbed an \"emotion picture\" by Monáe. It follows Monáe's character, Jane 57821, as she attempts to break free from the constraints of \"a totalitarian society [that] forcibly makes Jane comply with its homophobic beliefs... In the film, Monáe's character is trying to assert her individuality, which makes her the enemy of a soulless regime – a common tension in dystopian sci-fi.\" Actress Tessa Thompson and actor Jayson Aaron co-star as Zen and Ché respectively, lovers with whom Jane escapes \"the clutches of this repressive society.\" The film features little dialogue outside of the overlay of the album's songs that function as the narrative's main driving force. Tim Grierson of Rolling Stone described that, in the film, \"Monáe plays with the conventions and totems of dystopian sci-fi to speak her truth and promote a cultural shift toward a more inclusive and loving society – no matter what repressive government (whether real or fictional) is trying to crush that spirit. Monáe is speaking to the present, but for her, the future is now.\" On February 1, 2019, Monáe released a new director's cut of the Dirty Computer emotion picture with 13 extra minutes of bonus interviews from the cast and crew. ==Singles== On February 23, 2018, Monáe released \"Make Me Feel\" and \"Django Jane\", the first two singles from Dirty Computer. On April 11, the album's third single \"Pynk\", featuring Canadian musician Grimes, was released. \"I Like That\" was released on April 23 as a promotional single, but soon after in August, was made to be the final official single from the album. Three remix singles were released for the song on August 17, August 24, and November 9, 2018. Standalone music videos were also released for \"Crazy, Classic, Life\" and \"Screwed\" on December 12, 2018 and January 7, 2019 respectively. ==Critical reception== Dirty Computer received widespread acclaim from music critics. At Metacritic, which assigns a normalized rating out of 100 to reviews from mainstream critics, the album has an average score of 87 based on 33 reviews, indicating \"universal acclaim\". At AnyDecentMusic?, which gives a formulated rating on a 10-point scale, the album has an aggregate score of 8.4 based on 32 reviews. Reviewing the album for AllMusic, Andy Kellman said that \"While this is easily the most loaded Monáe album in terms of guests, with Brian Wilson, Stevie Wonder, and Grimes among the contributors, there's no doubt that it's a Wondaland product. It demonstrates that artful resistance and pop music are not mutually exclusive.\" Neil McCormick for The Daily Telegraph called Dirty Computer \"unblushingly and unsparingly direct...[It] establishes itself as a contender for album of the year, in more ways than one... [Monáe]'s layered sound is as contemporary as that of such digital trailblazers as Kendrick Lamar or Kanye West, yet it has an old-fashioned organic quality that comes from a bedrock of live musicianship. Dirty Computer sounds like 2018 distilled into a sci-fi funk pop extravaganza by a female Black Panther.\" The Independent reviewer Roisin O'Connor stated that Dirty Computer is \"a record that will go down as a milestone not just as a work of art in its own right, but as the perfect celebration of queerness, female power, and self-worth.\" Danette Chavez of The A.V. Club described that, on Dirty Computer, \"the erstwhile \"Electric Lady\" loses the metal and circuitry, but none of her power or artistry, cementing her status alongside Prince in the hall of hyper-talented, gender-fluid icons who love and promote blackness.\" Josh Hurst for Flood Magazine wrote that \"Every generation needs its own soundtrack for kicking against the pricks, and Monáe delivers one—easily the most pop-conversant, hook-laden, and propulsive music of her career.\" Robert Christgau, who in the past had found Monáe's voice too thin and her songwriting overly intellectual, was converted by Dirty Computers Prince-inspired, sex-positive songs: \"Too often prosex albums are shallow. While remaining intellectual, this one is more personal than the android dared.\" In a less enthusiastic review, Alexis Petridis of The Guardian suggested that \"You occasionally wonder if an understandable desire to cross over commercially might not be at the root of the album's less inspired moments: there's something commonplace and risk-averse about the pop-R\u0026B; backing of \"Crazy, Classic, Life\" and \"I Got the Juice\"... It's hard not to wonder if her failure to connect with a mass audience might be because her desire to work with concepts and characters, rather than unburden herself, suggests a certain aloofness... She is as elusive as ever, and her mystery remains intact. Without a true loosening of her poise, her position on the margins of pop could remain intact as well.\" Zachary Hoskins from Slant Magazine declared that \"while the songs here are consistently hooky, they lack the earlier albums' sonic adventurousness... While Monáe's heart is in the right place, her lyrics occasionally suffer from the clumsiness endemic to politicized pop... If Dirty Computer feels somewhat blunter and less ambitious than Monáe's earlier work, though, it's also her most immediately satisfying.\" ===Accolades=== The album was nominated for Album of the Year and the music video for \"Pynk\" was nominated for Best Music Video at the 61st Grammy Awards. The associated narrative film was a finalist for the 2019 Hugo Awards for Best Dramatic Presentation — Short Form. {| class=\"wikitable sortable\" |+Year-end lists |- ! Publication ! Accolade ! Rank |- | The Associated Press | AP’s Top 2018 Albums | style=\"text-align:center;\"| 1 |- | Complex | The Best Albums of 2018 | style=\"text-align:center;\"| 13 |- | Consequence of Sound | The Top 50 Albums of 2018 | style=\"text-align:center;\"| 2 |- | Entertainment Weekly | The 20 Best Albums of 2018 | style=\"text-align:center;\"| 9 |- | Flavorwire | The 25 Best Albums of 2018 | style=\"text-align:center;\"| 13 |- | The Guardian | The Best Albums of 2018 | style=\"text-align:center;\"| 3 |- | Independent | Albums of the Year 2018 | style=\"text-align:center;\"| 3 |- | The New York Times | The 28 Best Albums of 2018 | style=\"text-align:center;\"| 1 |- | NPR | The 50 Best Albums of 2018 | style=\"text-align:center;\"| 1 |- | Paste | The 50 Best Albums of 2018 | style=\"text-align:center;\"| 14 |- | The Ringer | The Best Albums of 2018 | style=\"text-align:center;\"| 5 |- | Rolling Stone | 50 Best Albums of 2018 | style=\"text-align:center;\"| 13 |- | The Skinny | Albums of 2018 | style=\"text-align:center;\"| 7 |- | Stereogum | The 50 Best Albums of 2018 | style=\"text-align:center;\"| 38 |- | Time | The 10 Best Albums of 2018 | style=\"text-align:center;\"| 2 |- | UNCUT | 75 Best Albums of 2018 | style=\"text-align:center;\"| 6 |- | Uproxx | The 50 Best Albums of 2018 | style=\"text-align:center;\"| 4 |} ==Commercial performance== Dirty Computer debuted at number six on the US Billboard 200, opening with 54,000 album- equivalent units in its first week, with 41,000 coming from pure sales. ==Track listing== Credits adapted from the album's liner notes Notes * signifies a co-producer * signifies an additional producer * signifies an additional vocal producer Sample credits * \"Django Jane\" contains a sample from \"A Dream\", written and performed by David Axelrod. * \"Pynk\" contains a sample from \"Pink\", written by Glen Ballard, Richard Supa and Steven Tyler. ==Personnel== Musicians * Janelle Monáe – lead vocals , background vocals * Zoë Kravitz – lead vocals * Pharrell Williams – lead vocals * Nate \"Rocket\" Wonder – background vocals , synthesizer , drums , programming , guitar , bass , piano * Brian Wilson – background vocals * The Skunks – background vocals * Matt Jardine – background vocals * Grimes – background vocals * Wynne Bennett – background vocals , synthesizer , drums , guitar * Nana Kwabena – background vocals , synthesizer , drums , programming * Sleepy Brown – background vocals * Todd Bergman – background vocals * Roman GianArthur – background vocals , drums , guitar * Isis Valentino – background vocals * Reverend Sean McMillan – \"love sermon\" * Stevie Wonder – \"oratory blessings\" * Jon Brion – synthesizer , mallets * Jon Jon Traxx – programming , guitar , bass , percussion * Kellindo – guitar * The Wondaland ArchOrchestra ** Alexander Page – violins ** Grace Shim – cello * Sounwave – additional drums * Thundercat – bass * Dennis Hamm – piano * Andrew Horowitz – synthesizer * Organized Noize – additional instrumentation Technical * Janelle Monáe – recording , vocal production , additional vocal production * Nate \"Rocket\" Wonder – recording , vocal production * Riggs Morales - A\u0026R; * Jon Brion – recording * Marco Sonzini – recording * Wynne Bennett – recording * Organized Noize – recording * Todd Bergman – additional engineering * Mick Guzauski – mixing * Serban Ghenea – mixing * John Hanes – engineering for mix * Dave Kutch – mastering * Justin Tranter – vocal production * Julia Michaels – vocal production * Chuck Lightning – additional vocal production Arrangement * Nate \"Rocket\" Wonder – arrangement , string arrangement * Roman GianArthur – arrangement * Nana Kwabena – arrangement * Wynne Bennett – arrangement * Janelle Monáe – musical arrangement * Jidenna – additional arrangement Design * Chuck Lightning – creative direction * The Wondaland Arts Society – creative direction * Joe Perez – art direction * Jenna Marsh – studio album art direction * Free Marseille – design * Abdul Ali – design * JUCO – photography ==Charts== {| class=\"wikitable sortable plainrowheaders\" style=\"text-align:center\" |- ! scope=\"col\"| Chart (2018) ! scope=\"col\"| Peak position |- |- |- |- |- |- |- |- |- |- |- |- ! scope=\"row\"| Japanese Albums (Oricon) | 133 |- |- |- |- |- ! scope=\"row\"| Spanish Albums (PROMUSICAE) | 33 |- |- |- ! scope=\"row\"| US Billboard 200 | 6 |- |} ==References== Category:2018 albums Category:Albums produced by Jon Brion Category:Albums produced by Mattman \u0026 Robin Category:Atlantic Records albums Category:Bad Boy Records albums Category:Janelle Monáe albums Category:Pop albums by American artists Category:Contemporary R\u0026B; albums by American artists Category:Funk albums by American artists Category:Hip hop albums by American artists Category:Neo soul albums Category:Concept albums Category:LGBT-related music ",
  "title": "Dirty Computer"
}},{"_index":"wikipedia","_type":"_doc","_id":"qiTVSZoBq7KcDrev0YH0","_score":15.682297,"_source":{
  "content": "In telecommunications, echo is the local display of data, either initially as it is locally sourced and sent, or finally as a copy of it is received back from a remote destination. Local echo is where the local sending equipment displays the outgoing sent data. Remote echo is where the display is a return copy of data as received remotely. Both are used together in a computed form of error detection to ensure that data received at the remote destination of a telecommunication are the same as data sent from the local source (a/k/a echoplex, echo check, or loop check). When (two) modems communicate in echoplex mode the remote modem echoes whatever it receives from the local modem. == Terminological confusion: echo is not duplex == A displayed 'echo' is independent of 'duplex' (or any) telecommunications transmission protocol. Probably from technical ignorance, \"half-duplex\" and \"full-duplex\" are used as slang for 'local echo' (a/k/a echo on) and 'remote echo', respectively, as typically they accompany one another. Strictly incorrect, this causes confusion (see duplex). Typically 'local echo' accompanies half-duplex transmission, which effectively doubles channel bandwidth by not repeating (echoing) data back from its destination (remote), as is reserved-for with 'full duplex' (which has only half of the bandwidth of 'half duplex'). Half- duplex can be set to 'echo off' for no echo at all. One example of 'local echo' used together with 'remote echo' (requires full-duplex) is for error checking pairs of data characters or chunks (echoplex) ensuring their duplicity (or else its just an extraneous annoyance). Similarly, for another example, in the case of the TELNET communications protocol a local echo protocol operates on top of a full-duplex underlying protocol. The TCP connection over which the TELNET protocol is layered provides a full-duplex connection, with no echo, across which data may be sent in either direction simultaneously. Whereas the Network Virtual Terminal that the TELNET protocol itself incorporates is a half-duplex device with (by default) local echo. == The devices that echo locally == Terminals are one of the things that may perform echoing for a connection. Others include modems, some form of intervening communications processor, or even the host system itself. For several common computer operating systems, it is the host system itself that performs the echoing, if appropriate (which it isn't for, say, entry of a user password when a terminal first connects and a user is prompted to log in). On OpenVMS, for example, echoing is performed as necessary by the host system. Similarly, on Unix-like systems, local echo is performed by the operating system kernel's terminal device driver, according to the state of a device control flag, maintained in software and alterable by applications programs via an `ioctl()` system call. The actual terminals and modems connected to such systems should have their local echo facilities switched off (so that they operate in no echo mode), lest passwords be locally echoed at password prompts, and all other input appear echoed twice. This is as true for terminal emulator programs, such as C-Kermit, running on a computer as it is for real terminals. == Controlling local echo == === Terminal emulators === Most terminal emulator programs have the ability to perform echo locally (which sometimes they misname \"half-duplex\"): * In the C-Kermit terminal emulator program, local echo is controlled by the `SET TERMINAL ECHO` command, which can be either `SET TERMINAL ECHO LOCAL` (which enables local echoing within the terminal emulator program itself) or `SET TERMINAL ECHO REMOTE` (where disables local echoing, leaving that up to another device in the communications channel—be that the modem or the remote host system—to perform as appropriate). * In ProComm it is the combination, which is a hot key that may be used at any time to toggle local echo on and off. * In the Terminal program that came with Microsoft Windows 3.1, local echo is controlled by a checkbox in the \"Terminal Preferences\" dialogue box accessed from the menu of the terminal program's window. === Modems === The Hayes commands that control local echo (in command mode) are for off and for on. For local echo (in data mode), the commands are and respectively. Note the reversal of the suffixed digits. Unlike the \"\" commands, the \"\" commands are not part of the EIA/TIA-602 standard. === Host systems === Some host systems perform local echo themselves, in their device drivers and so forth. *In Unix and POSIX- compatible systems, local echo is a flag in the POSIX terminal interface, settable programmatically with the `tcsetattr()` function. The echoing is performed by the operating system's terminal device (in some way that is not specified by the POSIX standard). The standard utility program that alters this flag programmatically is the `stty` command, using which the flag may be altered from shell scripts or an interactive shell. The command to turn local echo (by the host system) on is `stty echo` and the command to turn it off is `stty -echo`. *On OpenVMS systems, the operating system's terminal driver normally performs echoing. The terminal characteristic that controls whether it does this is the `ECHO` characteristic, settable with the DCL command `SET TERMINAL /ECHO` and unsettable with `SET TERMINAL /NOECHO`. == Footnotes == == References == === What supports what === === Sources used === * * * * * * * * * * * * * * * * * * * * * * * * * * Category:Error detection and correction Category:Modems Category:Data transmission ",
  "title": "Echo (computing)"
}},{"_index":"wikipedia","_type":"_doc","_id":"pCTVSZoBq7KcDrev4oOz","_score":15.682297,"_source":{
  "content": "The English language is sometimes described as the lingua franca of computing. In comparison to other sciences, where Latin and Greek are the principal sources of vocabulary, computer science borrows more extensively from English. Due to the technical limitations of early computers, and the lack of international standards on the Internet, computer users were limited to using English and the Latin alphabet. However, this historical limitation is less present today. Most software products are localized in numerous languages and the use of the Unicode character encoding has resolved problems with non-Latin alphabets. Some limitations have only been changed recently, such as with domain names, which previously allowed only ASCII characters. English is seen as having this role due to the prominence of the United States and the United Kingdom, both English-speaking countries, in development and popularization of computer systems, computer networks, software and information technology. ==Influence on other languages== The computing terminology of many languages borrows from English. Some language communities resist actively to that trend, and in other cases English is used extensively and more directly. This section gives some examples for the use of English terminology in other languages, and also mentions any notable differences. ===Bulgarian=== Both English and Russian have influence over Bulgarian computing vocabulary. However, in many cases the borrowed terminology is translated, and not transcribed phonetically. Combined with the use of Cyrillic this can make it difficult to recognize loanwords. For example, the Bulgarian term for motherboard is (IPA or literally \"bottom board\"). * – computer * – hard disk * – floppy disk; like the French disquette * – (phonetic) web site; but also \"\" – internet page ===Faroese=== The Faroese language has a sparse scientific vocabulary based on the language itself. Many Faroese scientific words are borrowed and/or modified versions of especially Nordic and English equivalents. The vocabulary is constantly evolving and thus new words often die out, and only a few survive and become widely used. Examples of successful words include e.g. \"telda\" (computer), \"kurla\" (at sign) and \"ambætari\" (server). ===French=== In French, there are some generally accepted English loan-words, but there is also a distinct effort to avoid them. In France, the Académie française is responsible for the standardisation of the language and often coins new technological terms. Some of them are accepted in practice, in other cases the English loanwords remain predominant. In Quebec, the Office québécois de la langue française has a similar function. * email/mail (in Europe); courriel (mainly in French-speaking Canada, but increasingly used in French-speaking Europe); mél. (only used as an abbreviation, similar to \"tél.\"\"Questions de langue\" on the Académie Française's website); more formally courrier électronique * pourriel – spam * hameçonnage, phishing – phishing * télécharger – to download * site web – website * lien, hyperlien – website hyper-link * base de données – database * caméra web, webcaméra, short webcam – webcam * amorcer, démarrer, booter – to boot * redémarrer, rebooter – to reboot * arrêter, éteindre – to shut down * amorçable, bootable – bootable * surfréquençage, surcadençage, overclocking – overclocking * refroidissement à l'eau – watercooling * tuning PC – case modding === German === In German, English words are very often used as well: * nouns: Computer, Website, Software, E-Mail, Blog * verbs: downloaden, booten, crashen === Japanese === See also: Japanese input methods Japanese uses the katakana alphabet for foreign loan words, a wide variety of which are in use today. English computing terms are remain prevalent in modern Japanese vocabulary. * コンピューター (konpyuuta) - computer * コーダー (koodaa) - coder * コーデック (koddekku) - codec * リンク (rinku) - link Utilizing a keyboard layout suitable for romanization of Japanese, a user may type in the Latin script in order to display Japanese, inclusive of hiragana, katakana, and Japanese kanji. \u003e Usually when writing in Japanese on a computer keyboard, the text is input \u003e in roman transcription, optionally according to Hepburn, Kunrei, or Nippon \u003e romanization; the common Japanese word processing programs allow for all \u003e three. Long vowels are input according to how they are written in kana; for \u003e example, a long o is input as ou, instead of an o with a circumflex or \u003e macron (ô or ō). As letters are keyed in, they are automatically converted, \u003e as specified, into either hiragana or katakana. And these kana phrases are \u003e in turn converted, as desired, into kanji. === Icelandic === The Icelandic language has its own vocabulary of scientific terms, still English borrowings exist. English or Icelandicised words are mostly used in casual conversations, whereas the Icelandic words might be longer or not widespread. === Norwegian === It's quite common to use English words in regards to computing in all Scandinavian languages. nouns: , , mail, software, , spam verbs: å boote, å spamme, å blogge === Polish === Polish language words derived from English: * dżojstik: joystick * kartrydż, kartridż: cartridge * interfejs: interface * mejl: e-mail === Russian === * History of computer hardware in Soviet Bloc countries * Computer Russification === Spanish === The English influence on the software industry and the internet in Latin America has borrowed significantly from the Castilian lexicon. ;Frequently untranslated, and their Spanish equivalent *email: correo electrónico *mouse (only in Latin America): ratón (mainly in Spain) *messenger: mensajero *webcam: cámara web *website: página web, sitio web *blog: bitácora, 'blog' *ban/banned: vetar, vetado *web: red ;Not translated *flog ;Undecided Many computing terms in Spanish share a common root with their English counterpart. In these cases, both terms are understood, but the Spanish is preferred for formal use: *link vs enlace or vínculo *net vs red == Character encoding == The early computer software and hardware had very little support for alphabets other than the Latin. As a result of this it was difficult or impossible to represent languages based on other scripts. The ASCII character encoding, created in the 1960s, only supported 128 different characters. With the use of additional software it was possible to provide support for some languages, for instance those based on the Cyrillic alphabet. However, complex-script languages like Chinese or Japanese need more characters than the 256 limit imposed by 8-bit character encodings. Some computers created in the former USSR had native support for the Cyrillic alphabet. The wide adoption of Unicode, and UTF-8 on the web, resolved most of these historical limitations. ASCII remains the de facto standard for command interpreters, programming languages and text-based communication protocols. * Mojibake – Text presented as \"unreadable\" when software fails due to character encoding issues. == Programming language == The syntax of most programming languages uses English keywords, and therefore it could be argued some knowledge of English is required in order to use them. However, it is important to recognize all programming languages are in the class of formal languages. They are very different from any natural language, including English. Some examples of non-English programming languages: * Arabic: ARLOGO, قلب * Bengali: BangaBhasha * Chinese: Chinese BASIC * Dutch: Superlogo * French: LSE, WinDev, Pascal (although the English version is more widespread) * Hebrew: Hebrew Programming Language * Icelandic: Fjölnir * Indian Languages: Hindawi Programming System * Russian: Glagol * Spanish: Lexico == Communication protocols == Many application protocols use text strings for requests and parameters, rather than the binary values commonly used in lower layer protocols. The request strings are generally based on English words, although in some cases the strings are contractions or acronyms of English expressions, which renders them somewhat cryptic to anyone not familiar with the protocol, whatever their proficiency in English. Nevertheless, the use of word-like strings is a convenient mnemonic device that allows a person skilled in the art (and with sufficient knowledge of English) to execute the protocol manually from a keyboard, usually for the purpose of finding a problem with the service. Examples: * FTP: USER, PASS (password), PASV (passive), PORT, RETR (retrieve), STOR (store), QUIT * SMTP: HELO (hello), MAIL, RCPT (recipient), DATA, QUIT * HTTP: GET, PUT, POST, HEAD (headers), DELETE, TRACE, OPTIONS It is notable that response codes, that is, the strings sent back by the recipient of a request, are typically numeric: for instance, in HTTP (and some borrowed by other protocols) * 200 OK request succeeded * 301 Moved Permanently to redirect the request to a new address * 404 Not Found the requested page does not exist This is because response codes also need to convey unambiguous information, but can have various nuances that the requester may optionally use to vary its subsequent actions. To convey all such \"sub-codes\" with alphabetic words would be unwieldy, and negate the advantage of using pseudo-English words. Since responses are usually generated by software they do not need to be mnemonic. Numeric codes are also more easily analyzed and categorized when they are processed by software, instead of a human testing the protocol by manual input. == Localization == === BIOS === Many personal computers have a BIOS chip, displaying text in English during boot time. === Keyboard shortcut === Keyboard shortcuts are usually defined in terms of English keywords such as CTRL+F for find. == English on the World Wide Web == English is the largest language on the World Wide Web, with 27% of internet users. === English speakers === Web user percentages usually focus on raw comparisons of the first language of those who access the web. Just as important is a consideration of second- and foreign-language users; i.e., the first language of a user does not necessarily reflect which language he or she regularly employs when using the web. ==== Native speakers ==== English-language users appear to be a plurality of web users, consistently cited as around one-third of the overall (near one billion). This reflects the relative affluence of English-speaking countries and high Internet penetration rates in them. This lead may be eroding due mainly to a rapid increase of Chinese users.English grip on internet being eroded | Technology | Guardian Unlimited First-language users among other relatively affluent countries appear generally stable, the two largest being German and Japanese, which each have between 5% and 10% of the overall share. === World Wide Web content === One widely quoted figure for the amount of web content in English is 80%.What percentage of the internet is in English? Other sources show figures five to fifteen points lower, though still well over 50%.English could snowball on Net TRN 112101 There are two notable facts about these percentages: The English web content is greater than the number of first- language English users by as much as 2 to 1. Given the enormous lead it already enjoys and its increasing use as a lingua franca in other spheres, English web content may continue to dominate even as English first-language Internet users decline. This is a classic positive feedback loop: new Internet users find it helpful to learn English and employ it online, thus reinforcing the language's prestige and forcing subsequent new users to learn English as well. Certain other factors (some predating the medium's appearance) have propelled English into a majority web-content position. Most notable in this regard is the tendency for researchers and professionals to publish in English to ensure maximum exposure. The largest database of medical bibliographical information, for example, shows English was the majority language choice for the past forty years and its share has continually increased over the same period.Language and country preponderance trends in MEDLINE and its causes The fact that non-Anglophones regularly publish in English only reinforces the language's dominance. English has a rich technical vocabulary (largely because native and non-native speakers alike use it to communicate technical ideas) and many IT and technical professionals use English regardless of country of origin (Linus Torvalds, for instance, comments his code in English, despite being from Finland and having Swedish as his first language). ==Notes== Category:English language Category:Computing and society Category:Internet culture Category:Natural language and computing Category:English as a global language ",
  "title": "English in computing"
}},{"_index":"wikipedia","_type":"_doc","_id":"WiTYSZoBq7KcDrevCLMD","_score":15.682297,"_source":{
  "content": "In computer science, overhead is any combination of excess or indirect computation time, memory, bandwidth, or other resources that are required to perform a specific task. It is a special case of engineering overhead. Overhead can be a deciding factor in software design, with regard to structure, error correction, and feature inclusion. Examples of computing overhead may be found in functional programming, data transfer, and data structures. ==Software design== ===Choice of implementation=== A programmer/software engineer may have a choice of several algorithms, encodings, data types or data structures, each of which have known characteristics. When choosing among them, their respective overhead should also be considered. ===Tradeoffs=== In software engineering, overhead can influence the decision whether or not to include features in new products, or indeed whether to fix bugs. A feature that has a high overhead may not be included – or needs a big financial incentive to do so. Often, even though software providers are well aware of bugs in their products, the payoff of fixing them is not worth the reward, because of the overhead. For example, an implicit data structure or succinct data structure may provide low space overhead, but at the cost of slow performance (space/time tradeoff). ===Run- time complexity of software=== Algorithmic complexity is generally specified using Big O Notation. This makes no comment on how long something takes to run or how much memory it uses, but how its increase depends on the size of the input. Overhead is deliberately not part of this calculation, since it varies from one machine to another, whereas the fundamental running time of an algorithm does not. This should be contrasted with algorithmic efficiency, which takes into account all kinds of resources – a combination (though not a trivial one) of complexity and overhead. ==Examples== ===Computer Programming (run-time and computational overhead)=== Invoking a function introduces a small run-time overhead. Sometimes the compiler can minimize this overhead by inlining some of these function calls. ===CPU Caches=== In a CPU cache, the \"cache size\" (or capacity) refers to how much data a cache stores. For instance, a \"4KB cache\" is a cache that holds 4KB of data. The \"4KB\" in this example excludes overhead bits such as frame, address, and tag information. Presentation for course in Computer Architecture. ===Communications (data transfer overhead)=== Reliably sending a payload of data over a communications network requires sending more than just payload itself. It also involves sending various control and signalling data (TCP) required to reach the destination. This creates a so-called protocol overhead as the additional data does not contribute to the intrinsic meaning of the message.Common Performance Issues in Network Applications Part 1: Interactive Applications, Windows XP Technical Articles, MicrosoftProtocol Overhead in IP/ATM Networks, Minnesota Supercomputer Center In telephony, number dialing and call set-up time are overheads. In 2-way (but half-duplex) radios, the use of \"over\" and other signalling needed to avoid collisions is an overhead. Protocol overhead can be expressed as a percentage of non-application bytes (protocol and frame synchronization) divided by the total number of bytes in the message. ===Encodings and data structures (size overhead)=== The encoding of information and data introduces overhead too. The date and time \"2011-07-12 07:18:47\" can be expressed as Unix time with the 32-bit signed integer `1310447927`, consuming only 4 bytes. Represented as ISO 8601 formatted UTF-8 encoded string `2011-07-12 07:18:47` the date would consume 19 bytes, a size overhead of 375% over the binary integer representation. As XML this date can be written as follows with an overhead of 218 characters, while adding the semantic context that it is a CHANGEDATE with index 1. 2011 07 12 07 18 47 The 349 bytes, resulting from the UTF-8 encoded XML, correlates to a size overhead of 8625% over the original integer representation. ==See also== *Rule of least power *Universal Turing machine ==References== Category:Software engineering ",
  "title": "Overhead (computing)"
}}]}}
